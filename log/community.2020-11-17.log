Starting CommunityApplication on DESKTOP-QPE2857 with PID 13212 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 67ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 6ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$16455157] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$42933fd4] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 4043 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
failed to load elasticsearch nodes : org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: [{#transport#-1}{Q49tQshBQw-cq3KHtdzJ1Q}{127.0.0.1}{127.0.0.1:9300}]
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605594986938'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@7cd6a50d


Using generated security password: ebfca022-57c8-4bcd-bb2e-e1c1219806ae

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@4a0ec40e, org.springframework.security.web.context.SecurityContextPersistenceFilter@4fac0032, org.springframework.security.web.header.HeaderWriterFilter@590ae106, org.springframework.security.web.authentication.logout.LogoutFilter@3212e7ad, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@18a763a0, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@22ac0a91, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@6357c925, org.springframework.security.web.session.SessionManagementFilter@47003e9, org.springframework.security.web.access.ExceptionTranslationFilter@6bb75f5d, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@10bf2ce8]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.kafka.config.internalKafkaListenerEndpointRegistry'; nested exception is org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
Shutting down Quartz Scheduler
Scheduler communityScheduler_$_DESKTOP-QPE28571605594986938 shutting down.
Scheduler communityScheduler_$_DESKTOP-QPE28571605594986938 paused.
Scheduler communityScheduler_$_DESKTOP-QPE28571605594986938 shutdown complete.
Shutting down ExecutorService 'applicationTaskExecutor'
{dataSource-1} closing ...
{dataSource-1} closed
Stopping service [Tomcat]


Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
Application run failed
org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.kafka.config.internalKafkaListenerEndpointRegistry'; nested exception is org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:185)
	at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:53)
	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:360)
	at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:158)
	at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:122)
	at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:893)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.finishRefresh(ServletWebServerApplicationContext.java:163)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:552)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:316)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248)
	at com.bilibili.CommunityApplication.main(CommunityApplication.java:12)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
Starting CommunityApplication on DESKTOP-QPE2857 with PID 21560 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 59ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 8ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$f32e4d53] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$1f7c3bd0] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1762 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
failed to load elasticsearch nodes : org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: [{#transport#-1}{X4YC1JunQSiQ-10nHRJY3Q}{127.0.0.1}{127.0.0.1:9300}]
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605595291406'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@3da03408


Using generated security password: f645d16d-e267-4349-a474-9efb59790c93

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@56ed2035, org.springframework.security.web.context.SecurityContextPersistenceFilter@1c796530, org.springframework.security.web.header.HeaderWriterFilter@d3a2668, org.springframework.security.web.authentication.logout.LogoutFilter@2769404c, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@443bc9d3, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@4444c4a7, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@47de0e83, org.springframework.security.web.session.SessionManagementFilter@451e4d49, org.springframework.security.web.access.ExceptionTranslationFilter@5ddaea27, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@457f7cb5]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
[Consumer clientId=consumer-1, groupId=community-consumer-group] Connection to node -1 could not be established. Broker may not be available.
Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.kafka.config.internalKafkaListenerEndpointRegistry'; nested exception is org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
Shutting down Quartz Scheduler
Scheduler communityScheduler_$_DESKTOP-QPE28571605595291406 shutting down.
Scheduler communityScheduler_$_DESKTOP-QPE28571605595291406 paused.
Scheduler communityScheduler_$_DESKTOP-QPE28571605595291406 shutdown complete.
Shutting down ExecutorService 'applicationTaskExecutor'
{dataSource-1} closing ...
{dataSource-1} closed
Stopping service [Tomcat]


Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
Application run failed
org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.kafka.config.internalKafkaListenerEndpointRegistry'; nested exception is org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:185)
	at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:53)
	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:360)
	at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:158)
	at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:122)
	at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:893)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.finishRefresh(ServletWebServerApplicationContext.java:163)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:552)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:316)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248)
	at com.bilibili.CommunityApplication.main(CommunityApplication.java:12)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
Starting CommunityApplication on DESKTOP-QPE2857 with PID 14388 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 60ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 6ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$9b5eab05] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$c7ac9982] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1763 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605602040889'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@2b02085a


Using generated security password: 39f97d3e-80f2-474d-b118-267268d9ed42

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@707eeed, org.springframework.security.web.context.SecurityContextPersistenceFilter@f89d8c2, org.springframework.security.web.header.HeaderWriterFilter@46fefcfc, org.springframework.security.web.authentication.logout.LogoutFilter@1d070f98, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@4e3ca8e0, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@1e6a06c7, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@4fd1b2d9, org.springframework.security.web.session.SessionManagementFilter@6bcd0001, org.springframework.security.web.access.ExceptionTranslationFilter@22669da8, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@6e309b04]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 8
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 8
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 8
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
partitions assigned: [delete-0]
partitions assigned: [publish-0]
partitions assigned: [comment-0, like-0, follow-0]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605454155510"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605602040889 started.
Starting ProtocolHandler ["http-nio-8989"]
Handling 1 trigger(s) that missed their scheduled fire-time.
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 9.024 seconds (JVM running for 10.026)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-6, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions [delete-0]
partitions revoked: [delete-0]
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions [comment-0, like-0, follow-0]
partitions revoked: [comment-0, like-0, follow-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [publish-0]
partitions revoked: [publish-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 9
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 9
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 9
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 9
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
partitions assigned: [comment-0, like-0, follow-0]
partitions assigned: [publish-0]
partitions assigned: [delete-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Resetting offset for partition share-0 to offset 0.
partitions assigned: [share-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 17 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-17 16:34:25],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
Starting without optional epoll library
Starting without optional kqueue library
ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
生成长图成功: D:\wkHtmlToImage\wkhtmltopdf\bin\wkhtmltoimage --quality 75 www.bilibili.com D:\wkHtmlToImage\data\community\wk-image\8b4e245a55584c0a98cb1ddeeda7b01a.png
用户[0:0:0:0:0:0:0:1],在[2020-11-17 16:35:08],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
获取长图失败: D:\wkHtmlToImage\data\community\wk-image\8b4e245a55584c0a98cb1ddeeda7b01a\png (系统找不到指定的路径。)
用户[0:0:0:0:0:0:0:1],在[2020-11-17 16:35:25],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
获取长图失败: D:\wkHtmlToImage\data\community\wk-image\8b4e245a55584c0a98cb1ddeeda7b01a\png (系统找不到指定的路径。)
用户[0:0:0:0:0:0:0:1],在[2020-11-17 16:35:29],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
获取长图失败: D:\wkHtmlToImage\data\community\wk-image\8b4e245a55584c0a98cb1ddeeda7b01a\png (系统找不到指定的路径。)
用户[0:0:0:0:0:0:0:1],在[2020-11-17 16:35:44],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
获取长图失败: D:\wkHtmlToImage\data\community\wk-image\8b4e245a55584c0a98cb1ddeeda7b01a\png (系统找不到指定的路径。)
Starting CommunityApplication on DESKTOP-QPE2857 with PID 21516 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 54ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 8ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$f7a753aa] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$23f54227] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1603 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605602231888'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@82623bf


Using generated security password: 5d29607d-42d8-496d-9b8b-f4d6dffbe518

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@788e8320, org.springframework.security.web.context.SecurityContextPersistenceFilter@3186fb6b, org.springframework.security.web.header.HeaderWriterFilter@168f37db, org.springframework.security.web.authentication.logout.LogoutFilter@18416d53, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@4ae08fbb, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@4ef8e408, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@7464ca2, org.springframework.security.web.session.SessionManagementFilter@4bea4e30, org.springframework.security.web.access.ExceptionTranslationFilter@69816351, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@22adebeb]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 11
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

partitions assigned: [publish-0]
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605602040889"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605602231888 started.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 7.386 seconds (JVM running for 8.464)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [publish-0]
partitions revoked: [publish-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 12
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 12
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 12
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 12
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
partitions assigned: [delete-0]
partitions assigned: [share-0]
partitions assigned: [publish-0]
partitions assigned: [comment-0, like-0, follow-0]
Starting without optional epoll library
Starting without optional kqueue library
[任务取消] 没有需要刷新的帖子!
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 9 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-17 16:37:39],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
Starting CommunityApplication on DESKTOP-QPE2857 with PID 8016 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 53ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 9ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$c0c1cc7f] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$ed0fbafc] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1607 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userController': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'quniu.bucket.header.url' in value "${quniu.bucket.header.url}"
{dataSource-0} closing ...
Stopping service [Tomcat]


Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userController': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'quniu.bucket.header.url' in value "${quniu.bucket.header.url}"
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:380)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1411)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:592)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:843)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:877)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:316)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248)
	at com.bilibili.CommunityApplication.main(CommunityApplication.java:12)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49)
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'quniu.bucket.header.url' in value "${quniu.bucket.header.url}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:178)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:124)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:237)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:211)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.lambda$processProperties$0(PropertySourcesPlaceholderConfigurer.java:175)
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveEmbeddedValue(AbstractBeanFactory.java:851)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1189)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1168)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:593)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:90)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:374)
	... 22 common frames omitted
Starting CommunityApplication on DESKTOP-QPE2857 with PID 19036 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 63ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 8ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$df591f80] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$ba70dfd] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1587 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userController': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'quniu.bucket.header.url' in value "${quniu.bucket.header.url}"
{dataSource-0} closing ...
Stopping service [Tomcat]


Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userController': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'quniu.bucket.header.url' in value "${quniu.bucket.header.url}"
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:380)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1411)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:592)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:320)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:318)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:843)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:877)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:549)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:142)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:775)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:316)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1260)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1248)
	at com.bilibili.CommunityApplication.main(CommunityApplication.java:12)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49)
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'quniu.bucket.header.url' in value "${quniu.bucket.header.url}"
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:178)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:124)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:237)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:211)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.lambda$processProperties$0(PropertySourcesPlaceholderConfigurer.java:175)
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveEmbeddedValue(AbstractBeanFactory.java:851)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1189)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1168)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:593)
	at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:90)
	at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:374)
	... 22 common frames omitted
Starting CommunityApplication on DESKTOP-QPE2857 with PID 19368 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 59ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 7ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$b7335227] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$e38140a4] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1524 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605620423942'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@4dbc934f


Using generated security password: 3a429771-a1eb-498d-860a-2b5d377cf942

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@39761453, org.springframework.security.web.context.SecurityContextPersistenceFilter@59c073fd, org.springframework.security.web.header.HeaderWriterFilter@7f4bf71b, org.springframework.security.web.authentication.logout.LogoutFilter@35e3fd98, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@3a6f08fb, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@30c2075a, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@42a2625f, org.springframework.security.web.session.SessionManagementFilter@5066bc64, org.springframework.security.web.access.ExceptionTranslationFilter@6467ab09, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@7383921d]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 14
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

partitions assigned: [publish-0]
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605602231888"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605620423942 started.
Handling 1 trigger(s) that missed their scheduled fire-time.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 6.347 seconds (JVM running for 7.493)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [publish-0]
partitions revoked: [publish-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 15
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 15
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 15
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 15
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
partitions assigned: [share-0]
partitions assigned: [comment-0, like-0, follow-0]
partitions assigned: [delete-0]
partitions assigned: [publish-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 7 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
Starting without optional epoll library
Starting without optional kqueue library
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:53],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:57],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:41:57],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:08],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.login].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:09],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllConversations].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:10],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.MessageServiceImpl.getLastNotice].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.MessageServiceImpl.getNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.MessageServiceImpl.getLastNotice].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.MessageServiceImpl.getNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.MessageServiceImpl.getLastNotice].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:12],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:16],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:16],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:16],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:16],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:16],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:16],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllNotice].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:16],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:16],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:16],访问了[com.bilibili.service.impl.MessageServiceImpl.updateMessageStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:16],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:16],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.MessageServiceImpl.getLastNotice].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.MessageServiceImpl.getNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.MessageServiceImpl.getLastNotice].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.MessageServiceImpl.getNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.MessageServiceImpl.getLastNotice].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:19],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:20],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:20],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:20],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:20],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:20],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllNotice].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:20],访问了[com.bilibili.service.impl.MessageServiceImpl.updateMessageStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:20],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:20],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.MessageServiceImpl.getLastNotice].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.MessageServiceImpl.getNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.MessageServiceImpl.getLastNotice].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.MessageServiceImpl.getNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.MessageServiceImpl.getLastNotice].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:21],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
[任务取消] 没有需要刷新的帖子!
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:25],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:25],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:25],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:25],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:25],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:26],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:42:26],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:39],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:39],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:39],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:39],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:39],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:39],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:44:41],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
Starting CommunityApplication on DESKTOP-QPE2857 with PID 18572 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 66ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 7ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$ceff9b24] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$fb4d89a1] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1644 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605620702094'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@7598dd5d


Using generated security password: 68c27571-7c5a-41b0-be2d-dbd98878a1e8

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@3cf9996b, org.springframework.security.web.context.SecurityContextPersistenceFilter@14819a70, org.springframework.security.web.header.HeaderWriterFilter@441ecea3, org.springframework.security.web.authentication.logout.LogoutFilter@39ed2745, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@5fcbb82, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@4f79b241, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@3f0e7230, org.springframework.security.web.session.SessionManagementFilter@6914d631, org.springframework.security.web.access.ExceptionTranslationFilter@75e45acf, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@42c41dfb]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 17
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

partitions assigned: [comment-0, like-0, follow-0]
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605620423942"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605620702094 started.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 6.391 seconds (JVM running for 7.392)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [comment-0, like-0, follow-0]
partitions revoked: [comment-0, like-0, follow-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 18
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 18
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 18
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 18
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
partitions assigned: [share-0]
partitions assigned: [comment-0, like-0, follow-0]
partitions assigned: [publish-0]
partitions assigned: [delete-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 7 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
Starting without optional epoll library
Starting without optional kqueue library
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:17],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:29],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:29],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:29],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:29],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:29],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:29],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:29],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.UserServiceImpl.updateHeader].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:34],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:45:40],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
[任务取消] 没有需要刷新的帖子!
Starting CommunityApplication on DESKTOP-QPE2857 with PID 19608 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 57ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 8ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$232be0bc] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$4f79cf39] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1616 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605620896577'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@1d415f51


Using generated security password: 97e16fea-03ae-426f-a9b0-8117cfce3b93

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@3d895566, org.springframework.security.web.context.SecurityContextPersistenceFilter@7238db77, org.springframework.security.web.header.HeaderWriterFilter@23cdf557, org.springframework.security.web.authentication.logout.LogoutFilter@62b2b0ae, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@4eb7d02a, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@231e63c, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@569f2c6a, org.springframework.security.web.session.SessionManagementFilter@2cfb6762, org.springframework.security.web.access.ExceptionTranslationFilter@3b09107a, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@81ae790]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
Scheduler communityScheduler_$_DESKTOP-QPE28571605620896577 started.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 6.3 seconds (JVM running for 7.273)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 19
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 19
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 19
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 19
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
partitions assigned: [delete-0]
partitions assigned: [comment-0, like-0, follow-0]
partitions assigned: [publish-0]
partitions assigned: [share-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 7 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:23],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
Starting without optional epoll library
Starting without optional kqueue library
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:24],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605620702094"'s failed in-progress jobs.
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:28],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:28],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:28],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:28],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:28],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:28],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:28],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.UserServiceImpl.updateHeader].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.UserServiceImpl.findLoginTicket].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.UserServiceImpl.getAuthorities].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.DataServiceImpl.recordDau].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:38],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.LickServiceImpl.LinkStatus].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.MessageServiceImpl.getAllLettersUnreadCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-17 21:48:39],访问了[com.bilibili.service.impl.MessageServiceImpl.getUnreadNoticeCount].
[任务取消] 没有需要刷新的帖子!
