Starting CommunityApplication on DESKTOP-QPE2857 with PID 15544 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 49ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 6ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$a592db96] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$d1e0ca13] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 2820 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'eventConsumer': Unsatisfied dependency expressed through field 'taskScheduler'; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type 'org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}
{dataSource-0} closing ...
Stopping service [Tomcat]


Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.


***************************
APPLICATION FAILED TO START
***************************

Description:

Field taskScheduler in com.bilibili.event.EventConsumer required a bean of type 'org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler' that could not be found.

The injection point has the following annotations:
	- @org.springframework.beans.factory.annotation.Autowired(required=true)

The following candidates were found but could not be injected:
	- Bean method 'taskScheduler' in 'TaskSchedulingAutoConfiguration' not loaded because @ConditionalOnBean (names: org.springframework.context.annotation.internalScheduledAnnotationProcessor; SearchStrategy: all) did not find any beans named org.springframework.context.annotation.internalScheduledAnnotationProcessor


Action:

Consider revisiting the entries above or defining a bean of type 'org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler' in your configuration.

Starting CommunityApplication on DESKTOP-QPE2857 with PID 15740 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 59ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 8ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$9673e75a] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$c2c1d5d7] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1681 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'taskScheduler'
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605679161214'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@393eebf4


Using generated security password: 622c7e5a-ad74-4d23-9dfa-4d022abbad4a

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@3849fc8f, org.springframework.security.web.context.SecurityContextPersistenceFilter@1a48305c, org.springframework.security.web.header.HeaderWriterFilter@1792710a, org.springframework.security.web.authentication.logout.LogoutFilter@7ab77c18, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@12ae7748, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@2c8ab85, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@639aacb2, org.springframework.security.web.session.SessionManagementFilter@18186f73, org.springframework.security.web.access.ExceptionTranslationFilter@6c4c049c, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@2a5adf0a]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605620896577"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605679161214 started.
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
Handling 1 trigger(s) that missed their scheduled fire-time.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 7.241 seconds (JVM running for 8.221)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 22
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 22
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 22
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 22
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
partitions assigned: [publish-0]
partitions assigned: [share-0]
partitions assigned: [comment-0, like-0, follow-0]
partitions assigned: [delete-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 12 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
Starting without optional epoll library
Starting without optional kqueue library
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 13:59:34],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 14:00:29],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
生成长图成功: D:\wkHtmlToImage\wkhtmltopdf\bin\wkhtmltoimage --quality 75 https://www.qiniu.com D:\wkHtmlToImage\data\community\wk-image/7084c4c0b4e4495e99ebbcee59c2958f.png
等待图片生成[7084c4c0b4e4495e99ebbcee59c2958f].
等待图片生成[7084c4c0b4e4495e99ebbcee59c2958f].
等待图片生成[7084c4c0b4e4495e99ebbcee59c2958f].
等待图片生成[7084c4c0b4e4495e99ebbcee59c2958f].
等待图片生成[7084c4c0b4e4495e99ebbcee59c2958f].
开始第1次上传[7084c4c0b4e4495e99ebbcee59c2958f].
第1次上传失败[7084c4c0b4e4495e99ebbcee59c2958f].
开始第2次上传[7084c4c0b4e4495e99ebbcee59c2958f].
第2次上传成功[7084c4c0b4e4495e99ebbcee59c2958f].
[任务取消] 没有需要刷新的帖子!
Starting CommunityApplication on DESKTOP-QPE2857 with PID 988 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 67ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 7ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$d5ac1ff8] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$1fa0e75] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1771 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'taskScheduler'
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605679502392'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@1d6a577


Using generated security password: b607683e-eb13-4121-8aa7-d0b3876dbba9

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@2d66321e, org.springframework.security.web.context.SecurityContextPersistenceFilter@78f56480, org.springframework.security.web.header.HeaderWriterFilter@5e1108b8, org.springframework.security.web.authentication.logout.LogoutFilter@d37912, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@59d20584, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@3571057f, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@365f9ebe, org.springframework.security.web.session.SessionManagementFilter@35c47016, org.springframework.security.web.access.ExceptionTranslationFilter@d2017ac, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@21649d86]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 24
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

partitions assigned: [publish-0]
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605679161214"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605679502392 started.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 7.033 seconds (JVM running for 8.173)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [publish-0]
partitions revoked: [publish-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 25
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 25
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 25
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 25
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
partitions assigned: [share-0]
partitions assigned: [comment-0, like-0, follow-0]
partitions assigned: [delete-0]
partitions assigned: [publish-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 8 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-18 14:05:19],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
Starting without optional epoll library
Starting without optional kqueue library
ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
生成长图成功: D:\wkHtmlToImage\wkhtmltopdf\bin\wkhtmltoimage --quality 75 https://www.qiniu.com D:\wkHtmlToImage\data\community\wk-image/bc0493ff8b454eeca2630eac8e9327b1.png
等待图片生成[bc0493ff8b454eeca2630eac8e9327b1].
等待图片生成[bc0493ff8b454eeca2630eac8e9327b1].
等待图片生成[bc0493ff8b454eeca2630eac8e9327b1].
等待图片生成[bc0493ff8b454eeca2630eac8e9327b1].
开始第1次上传[bc0493ff8b454eeca2630eac8e9327b1].
第1次上传失败[bc0493ff8b454eeca2630eac8e9327b1].
开始第2次上传[bc0493ff8b454eeca2630eac8e9327b1].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 14:05:22],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
生成长图成功: D:\wkHtmlToImage\wkhtmltopdf\bin\wkhtmltoimage --quality 75 https://www.qiniu.com D:\wkHtmlToImage\data\community\wk-image/b524ff06d4b149d0b99b2ea15be1a537.png
等待图片生成[b524ff06d4b149d0b99b2ea15be1a537].
等待图片生成[b524ff06d4b149d0b99b2ea15be1a537].
等待图片生成[b524ff06d4b149d0b99b2ea15be1a537].
等待图片生成[b524ff06d4b149d0b99b2ea15be1a537].
等待图片生成[b524ff06d4b149d0b99b2ea15be1a537].
第2次上传成功[bc0493ff8b454eeca2630eac8e9327b1].
开始第1次上传[b524ff06d4b149d0b99b2ea15be1a537].
第1次上传失败[b524ff06d4b149d0b99b2ea15be1a537].
开始第2次上传[b524ff06d4b149d0b99b2ea15be1a537].
第2次上传成功[b524ff06d4b149d0b99b2ea15be1a537].
Starting CommunityApplication on DESKTOP-QPE2857 with PID 16656 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 69ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 8ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$f307ee2f] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$1f55dcac] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1787 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'taskScheduler'
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605692116957'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@6c130996


Using generated security password: 18999f56-69ea-4ff6-a9e1-1d33665c60b3

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@529582ae, org.springframework.security.web.context.SecurityContextPersistenceFilter@1bbb71f3, org.springframework.security.web.header.HeaderWriterFilter@6b5f4e56, org.springframework.security.web.authentication.logout.LogoutFilter@5c093c57, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@73b2e0d2, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@3b54924d, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@7034c78e, org.springframework.security.web.session.SessionManagementFilter@6900afd6, org.springframework.security.web.access.ExceptionTranslationFilter@44959958, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@60e5199e]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605679502392"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605692116957 started.
Handling 1 trigger(s) that missed their scheduled fire-time.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 7.518 seconds (JVM running for 8.594)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 28
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 28
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 28
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 28
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
partitions assigned: [share-0]
partitions assigned: [delete-0]
partitions assigned: [comment-0, like-0, follow-0]
partitions assigned: [publish-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 9 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:17],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
Starting without optional epoll library
Starting without optional kqueue library
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:17],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:17],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:17],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:35:18],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:10],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:26],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:36:47],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
[任务取消] 没有需要刷新的帖子!
[任务取消] 没有需要刷新的帖子!
Starting CommunityApplication on DESKTOP-QPE2857 with PID 16872 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 62ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 7ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$57e61808] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$84340685] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1719 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'taskScheduler'
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605692590281'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@58514968


Using generated security password: 0941d516-1b86-4474-b9ef-ad5628b11994

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@62ed4775, org.springframework.security.web.context.SecurityContextPersistenceFilter@67839726, org.springframework.security.web.header.HeaderWriterFilter@6614fa45, org.springframework.security.web.authentication.logout.LogoutFilter@63ad394e, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@3f346856, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@61466666, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@562cfd14, org.springframework.security.web.session.SessionManagementFilter@480953bd, org.springframework.security.web.access.ExceptionTranslationFilter@34b1e710, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@60b7f6a3]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 30
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
partitions assigned: [share-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605692116957"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605692590281 started.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 6.931 seconds (JVM running for 8.017)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [share-0]
partitions revoked: [share-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 31
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 31
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 31
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 31
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
partitions assigned: [comment-0, like-0, follow-0]
partitions assigned: [publish-0]
partitions assigned: [delete-0]
partitions assigned: [share-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 10 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
Starting without optional epoll library
Starting without optional kqueue library
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:42],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:42],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
服务器异常：参数错误!
com.bilibili.service.impl.DiscussPostServiceImpl$1.load(DiscussPostServiceImpl.java:65)
com.bilibili.service.impl.DiscussPostServiceImpl$1.load(DiscussPostServiceImpl.java:56)
com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache.lambda$new$0(BoundedLocalCache.java:3670)
com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2337)
java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1853)
com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2335)
com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2318)
com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:111)
com.github.benmanes.caffeine.cache.LocalLoadingCache.get(LocalLoadingCache.java:66)
com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost(DiscussPostServiceImpl.java:86)
com.bilibili.service.impl.DiscussPostServiceImpl$$FastClassBySpringCGLIB$$13abb811.invoke(<generated>)
org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:749)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
org.springframework.aop.framework.adapter.MethodBeforeAdviceInterceptor.invoke(MethodBeforeAdviceInterceptor.java:56)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:93)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:688)
com.bilibili.service.impl.DiscussPostServiceImpl$$EnhancerBySpringCGLIB$$d3e3bd3d.findAllDiscussPost(<generated>)
com.bilibili.controller.HomeController.index(HomeController.java:53)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:190)
org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138)
org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)
org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:892)
org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:797)
org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1039)
org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942)
org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005)
org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:897)
javax.servlet.http.HttpServlet.service(HttpServlet.java:634)
org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:882)
javax.servlet.http.HttpServlet.service(HttpServlet.java:741)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:320)
org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:119)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:74)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:215)
org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:178)
org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:357)
org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:270)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:92)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:200)
org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96)
org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:490)
org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139)
org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92)
org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343)
org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:408)
org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)
org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:836)
org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1747)
org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
java.lang.Thread.run(Thread.java:748)
Resolved [java.lang.IllegalArgumentException: 参数错误!]
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:45:42],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
[任务取消] 没有需要刷新的帖子!
[任务取消] 没有需要刷新的帖子!
Starting CommunityApplication on DESKTOP-QPE2857 with PID 16264 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 62ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 9ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$c2dce2a] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$387bbca7] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1761 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'taskScheduler'
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605693443813'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@10d6d270


Using generated security password: 3b25b4a4-0e1a-4893-ae3c-eb06071b64c6

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@5d7abe13, org.springframework.security.web.context.SecurityContextPersistenceFilter@44a033df, org.springframework.security.web.header.HeaderWriterFilter@2b660627, org.springframework.security.web.authentication.logout.LogoutFilter@4918ec4e, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@41ef580f, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@34ef581d, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@693870f4, org.springframework.security.web.session.SessionManagementFilter@e22220c, org.springframework.security.web.access.ExceptionTranslationFilter@1abc19df, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@756838de]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 33
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
partitions assigned: [delete-0]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
Scheduler communityScheduler_$_DESKTOP-QPE28571605693443813 started.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 7.007 seconds (JVM running for 8.115)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [delete-0]
partitions revoked: [delete-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 34
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 34
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 34
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 34
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
partitions assigned: [share-0]
partitions assigned: [publish-0]
partitions assigned: [delete-0]
partitions assigned: [comment-0, like-0, follow-0]
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605692590281"'s failed in-progress jobs.
ClusterManager: ......Freed 1 acquired trigger(s).
Starting without optional epoll library
Starting without optional kqueue library
[任务取消] 没有需要刷新的帖子!
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 9 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:40],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:42],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:42],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
服务器异常：参数错误!
com.bilibili.service.impl.DiscussPostServiceImpl$1.load(DiscussPostServiceImpl.java:66)
com.bilibili.service.impl.DiscussPostServiceImpl$1.load(DiscussPostServiceImpl.java:56)
com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache.lambda$new$0(BoundedLocalCache.java:3670)
com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2337)
java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1853)
com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2335)
com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2318)
com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:111)
com.github.benmanes.caffeine.cache.LocalLoadingCache.get(LocalLoadingCache.java:66)
com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost(DiscussPostServiceImpl.java:87)
com.bilibili.service.impl.DiscussPostServiceImpl$$FastClassBySpringCGLIB$$13abb811.invoke(<generated>)
org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:749)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
org.springframework.aop.framework.adapter.MethodBeforeAdviceInterceptor.invoke(MethodBeforeAdviceInterceptor.java:56)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:93)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:688)
com.bilibili.service.impl.DiscussPostServiceImpl$$EnhancerBySpringCGLIB$$dc6df680.findAllDiscussPost(<generated>)
com.bilibili.controller.HomeController.index(HomeController.java:53)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:190)
org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138)
org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)
org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:892)
org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:797)
org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1039)
org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942)
org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005)
org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:897)
javax.servlet.http.HttpServlet.service(HttpServlet.java:634)
org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:882)
javax.servlet.http.HttpServlet.service(HttpServlet.java:741)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:320)
org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:119)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:74)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:215)
org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:178)
org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:357)
org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:270)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:92)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:200)
org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96)
org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:490)
org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139)
org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92)
org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343)
org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:408)
org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)
org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:836)
org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1747)
org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
java.lang.Thread.run(Thread.java:748)
Resolved [java.lang.IllegalArgumentException: 参数错误!]
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:57:42],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
Starting CommunityApplication on DESKTOP-QPE2857 with PID 16760 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 61ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 7ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$a394ad5e] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$cfe29bdb] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1833 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'taskScheduler'
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605693498330'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@10585d54


Using generated security password: 02c6e195-1eca-4e95-a272-5dba459e90a0

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@695725fe, org.springframework.security.web.context.SecurityContextPersistenceFilter@17030380, org.springframework.security.web.header.HeaderWriterFilter@5b1b73cd, org.springframework.security.web.authentication.logout.LogoutFilter@7d0513fd, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@6dc2c78d, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@78ba304e, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@48792385, org.springframework.security.web.session.SessionManagementFilter@2e7f2aaf, org.springframework.security.web.access.ExceptionTranslationFilter@66391ba, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@1cb7fffa]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 36
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

partitions assigned: [share-0]
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605693443813"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605693498330 started.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 7.852 seconds (JVM running for 9.657)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [share-0]
partitions revoked: [share-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 37
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 37
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 37
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 37
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
partitions assigned: [delete-0]
partitions assigned: [publish-0]
partitions assigned: [share-0]
partitions assigned: [comment-0, like-0, follow-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 8 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
Starting without optional epoll library
Starting without optional kqueue library
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:22],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:24],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 17:58:24],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
[Consumer clientId=consumer-2, groupId=community-consumer-group] Group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery
[Consumer clientId=consumer-8, groupId=community-consumer-group] Group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery
[Consumer clientId=consumer-4, groupId=community-consumer-group] Group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery
[Consumer clientId=consumer-6, groupId=community-consumer-group] Group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery
服务器异常：参数错误!
com.bilibili.service.impl.DiscussPostServiceImpl$1.load(DiscussPostServiceImpl.java:66)
com.bilibili.service.impl.DiscussPostServiceImpl$1.load(DiscussPostServiceImpl.java:56)
com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache.lambda$new$0(BoundedLocalCache.java:3670)
com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2337)
java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1853)
com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2335)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:01:06],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery
com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2318)
com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:111)
com.github.benmanes.caffeine.cache.LocalLoadingCache.get(LocalLoadingCache.java:66)
com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost(DiscussPostServiceImpl.java:87)
com.bilibili.service.impl.DiscussPostServiceImpl$$FastClassBySpringCGLIB$$13abb811.invoke(<generated>)
org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:749)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
org.springframework.aop.framework.adapter.MethodBeforeAdviceInterceptor.invoke(MethodBeforeAdviceInterceptor.java:56)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:93)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:688)
com.bilibili.service.impl.DiscussPostServiceImpl$$EnhancerBySpringCGLIB$$3d3bf55f.findAllDiscussPost(<generated>)
com.bilibili.controller.HomeController.index(HomeController.java:53)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:190)
org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138)
org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)
org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:892)
org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:797)
org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1039)
org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942)
org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005)
org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:897)
javax.servlet.http.HttpServlet.service(HttpServlet.java:634)
org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:882)
javax.servlet.http.HttpServlet.service(HttpServlet.java:741)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53)
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:01:06],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:320)
org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:119)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
服务器异常：参数错误!
org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:74)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:215)
org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:178)
org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:357)
org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:270)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
com.bilibili.service.impl.DiscussPostServiceImpl$1.load(DiscussPostServiceImpl.java:66)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
com.bilibili.service.impl.DiscussPostServiceImpl$1.load(DiscussPostServiceImpl.java:56)
org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache.lambda$new$0(BoundedLocalCache.java:3670)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2337)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1853)
org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:92)
com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2335)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2318)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:111)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
com.github.benmanes.caffeine.cache.LocalLoadingCache.get(LocalLoadingCache.java:66)
org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93)
com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost(DiscussPostServiceImpl.java:87)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
com.bilibili.service.impl.DiscussPostServiceImpl$$FastClassBySpringCGLIB$$13abb811.invoke(<generated>)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:749)
org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.springframework.aop.framework.adapter.MethodBeforeAdviceInterceptor.invoke(MethodBeforeAdviceInterceptor.java:56)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:93)
org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:200)
org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96)
org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:688)
org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:490)
com.bilibili.service.impl.DiscussPostServiceImpl$$EnhancerBySpringCGLIB$$3d3bf55f.findAllDiscussPost(<generated>)
org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139)
com.bilibili.controller.HomeController.index(HomeController.java:53)
org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:408)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)
org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:190)
org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:836)
org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138)
org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1747)
org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)
org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)
org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:892)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:797)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1039)
java.lang.Thread.run(Thread.java:748)
org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:942)
org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1005)
org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:897)
javax.servlet.http.HttpServlet.service(HttpServlet.java:634)
org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:882)
javax.servlet.http.HttpServlet.service(HttpServlet.java:741)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:320)
org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:119)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:74)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
Resolved [java.lang.IllegalArgumentException: 参数错误!]
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:334)
org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:215)
org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:178)
org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:357)
org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:270)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:92)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:200)
org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193)
org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166)
org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:200)
org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96)
org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:490)
org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139)
org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92)
org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343)
org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:408)
org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66)
org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:836)
org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1747)
org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
java.lang.Thread.run(Thread.java:748)
Resolved [java.lang.IllegalArgumentException: 参数错误!]
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:01:06],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
Scheduler communityScheduler_$_DESKTOP-QPE28571605693498330 paused.
Shutting down ExecutorService
Shutting down ExecutorService
Shutting down ExecutorService
Shutting down ExecutorService
Consumer stopped
Consumer stopped
Consumer stopped
Consumer stopped
Shutting down Quartz Scheduler
Scheduler communityScheduler_$_DESKTOP-QPE28571605693498330 shutting down.
Scheduler communityScheduler_$_DESKTOP-QPE28571605693498330 paused.
Scheduler communityScheduler_$_DESKTOP-QPE28571605693498330 shutdown complete.
Shutting down ExecutorService 'applicationTaskExecutor'
Shutting down ExecutorService 'taskScheduler'
{dataSource-1} closing ...
{dataSource-1} closed
Starting CommunityApplication on DESKTOP-QPE2857 with PID 7212 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 62ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 8ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$33a9bcab] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$5ff7ab28] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1755 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'taskScheduler'
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605693681077'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@7427ee59


Using generated security password: 4ca61616-7a7c-46bc-82ce-16876ba2cfde

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@603177d5, org.springframework.security.web.context.SecurityContextPersistenceFilter@76d9066a, org.springframework.security.web.header.HeaderWriterFilter@7934746d, org.springframework.security.web.authentication.logout.LogoutFilter@2c329d9b, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@5b65cb87, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@6285a10, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@16c7d701, org.springframework.security.web.session.SessionManagementFilter@5bb1b28f, org.springframework.security.web.access.ExceptionTranslationFilter@7174b319, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@258fafab]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Cluster ID: QO7D4advRRq2h8kiil_eJw
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 39
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

partitions assigned: [publish-0]
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605693498330"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605693681077 started.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 7.083 seconds (JVM running for 8.193)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [publish-0]
partitions revoked: [publish-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 40
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 40
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 40
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 40
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
partitions assigned: [publish-0]
partitions assigned: [share-0]
partitions assigned: [delete-0]
partitions assigned: [comment-0, like-0, follow-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 8 ms
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
Starting without optional epoll library
Starting without optional kqueue library
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:20],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:21],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
[任务取消] 没有需要刷新的帖子!
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
load post list from DB.
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:33],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.DataServiceImpl.recordUV].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.DiscussPostServiceImpl.findAllDiscussPost].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.UserServiceImpl.getUserById].
用户[0:0:0:0:0:0:0:1],在[2020-11-18 18:02:35],访问了[com.bilibili.service.impl.LickServiceImpl.LikeCount].
[任务取消] 没有需要刷新的帖子!
Starting CommunityApplication on DESKTOP-QPE2857 with PID 4336 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 70ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 9ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$6d0db8e1] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$995ba75e] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1658 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'taskScheduler'
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605695869434'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@3260f094


Using generated security password: f7c65f7a-4a88-4fbd-ad9a-a55aa34d9f19

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@ae5f3bf, org.springframework.security.web.context.SecurityContextPersistenceFilter@2bbdd82d, org.springframework.security.web.header.HeaderWriterFilter@7eed82af, org.springframework.security.web.authentication.logout.LogoutFilter@8d1766d, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@616b2697, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@6cd6ec42, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@327e8d66, org.springframework.security.web.session.SessionManagementFilter@39aa8dc2, org.springframework.security.web.access.ExceptionTranslationFilter@29f719ea, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@7ba3e52c]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 42
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

partitions assigned: [comment-0, like-0, follow-0]
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605693681077"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605695869434 started.
Handling 1 trigger(s) that missed their scheduled fire-time.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 6.652 seconds (JVM running for 7.747)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [comment-0, like-0, follow-0]
partitions revoked: [comment-0, like-0, follow-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 43
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 43
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 43
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 43
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
partitions assigned: [publish-0]
partitions assigned: [comment-0, like-0, follow-0]
partitions assigned: [delete-0]
partitions assigned: [share-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 10 ms
Starting without optional epoll library
Starting without optional kqueue library
load post list from DB.
Starting CommunityApplication on DESKTOP-QPE2857 with PID 7464 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 68ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 7ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$82b894db] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$af068358] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1629 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'taskScheduler'
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605696059298'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@347f68f9


Using generated security password: f9369064-24ed-4bd7-bb19-db8d2140fb31

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@43b1d06e, org.springframework.security.web.context.SecurityContextPersistenceFilter@49d02714, org.springframework.security.web.header.HeaderWriterFilter@480af6cc, org.springframework.security.web.authentication.logout.LogoutFilter@4b9f1ad6, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@7bb3a3e0, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@3ccd12ca, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@6e0f9e09, org.springframework.security.web.session.SessionManagementFilter@6cc2e0cc, org.springframework.security.web.access.ExceptionTranslationFilter@2eff33f7, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@71fc93d0]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 45
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

partitions assigned: [comment-0, like-0, follow-0]
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605695869434"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605696059298 started.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 6.718 seconds (JVM running for 7.751)
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 10 ms
Starting without optional epoll library
Starting without optional kqueue library
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [comment-0, like-0, follow-0]
partitions revoked: [comment-0, like-0, follow-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 46
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 46
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 46
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 46
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
partitions assigned: [delete-0]
partitions assigned: [publish-0]
partitions assigned: [share-0]
partitions assigned: [comment-0, like-0, follow-0]
[任务取消] 没有需要刷新的帖子!
Starting CommunityApplication on DESKTOP-QPE2857 with PID 19064 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 68ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 6ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$3fbc777b] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$6c0a65f8] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 1576 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'taskScheduler'
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605696205459'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@12e8975b


Using generated security password: b7d15067-d6ed-4ef3-aa2c-3f8e357f686e

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@742edbc7, org.springframework.security.web.context.SecurityContextPersistenceFilter@5e52e624, org.springframework.security.web.header.HeaderWriterFilter@a357713, org.springframework.security.web.authentication.logout.LogoutFilter@8d00c1e, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@53361750, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@9e37c8c, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@53406953, org.springframework.security.web.session.SessionManagementFilter@2a1b7506, org.springframework.security.web.access.ExceptionTranslationFilter@5d5dc5a5, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@29f719ea]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 48
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

partitions assigned: [comment-0, like-0, follow-0]
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Cluster ID: QO7D4advRRq2h8kiil_eJw
Kafka version : 2.0.1
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
Scheduler communityScheduler_$_DESKTOP-QPE28571605696205459 started.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 6.575 seconds (JVM running for 7.711)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [comment-0, like-0, follow-0]
partitions revoked: [comment-0, like-0, follow-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 49
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 49
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 49
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 49
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
partitions assigned: [publish-0]
partitions assigned: [delete-0]
partitions assigned: [share-0]
partitions assigned: [comment-0, like-0, follow-0]
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605696059298"'s failed in-progress jobs.
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 9 ms
Starting without optional epoll library
Starting without optional kqueue library
load post list from DB.
load post list from DB.
Starting CommunityApplication on DESKTOP-QPE2857 with PID 17256 (started by 北极光。 in D:\编程\community)
No active profile set, falling back to default profiles: default
Devtools property defaults active! Set 'spring.devtools.add-properties' to 'false' to disable
For additional web related logging consider setting the 'logging.level.web' property to 'DEBUG'
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Finished Spring Data repository scanning in 73ms. Found 1 repository interfaces.
Multiple Spring Data modules found, entering strict repository configuration mode!
Bootstrapping Spring Data repositories in DEFAULT mode.
Spring Data Redis - Could not safely identify store assignment for repository candidate interface com.bilibili.mapper.ElasticsearchMapper.
Finished Spring Data repository scanning in 7ms. Found 0 repository interfaces.
Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$1b6cfee9] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$47baed66] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
Tomcat initialized with port(s): 8989 (http)
Initializing ProtocolHandler ["http-nio-8989"]
Starting service [Tomcat]
Starting Servlet engine: [Apache Tomcat/9.0.19]
Loaded APR based Apache Tomcat Native library [1.2.23] using APR version [1.7.0].
APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true].
APR/OpenSSL configuration: useAprConnector [false], useOpenSSL [true]
OpenSSL successfully initialized [OpenSSL 1.1.1c  28 May 2019]
Initializing Spring embedded WebApplicationContext
Root WebApplicationContext: initialization completed in 2023 ms
no modules loaded
loaded plugin [org.elasticsearch.index.reindex.ReindexPlugin]
loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
loaded plugin [org.elasticsearch.script.mustache.MustachePlugin]
loaded plugin [org.elasticsearch.transport.Netty4Plugin]
Adding transport node : 127.0.0.1:9300
LiveReload server is running on port 35729
Initializing ExecutorService 'taskScheduler'
Initializing ExecutorService 'applicationTaskExecutor'
Adding welcome page template: index
{dataSource-1} inited
Using default implementation for ThreadExecutor
Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
Quartz Scheduler v.2.3.1 created.
Using db table-based data access locking (synchronization).
JobStoreCMT initialized.
Scheduler meta-data: Quartz Scheduler (v2.3.1) 'communityScheduler' with instanceId 'DESKTOP-QPE28571605696304044'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 5 threads.
  Using job-store 'org.springframework.scheduling.quartz.LocalDataSourceJobStore' - which supports persistence. and is clustered.

Quartz scheduler 'communityScheduler' initialized from an externally provided properties instance.
Quartz scheduler version: 2.3.1
JobFactory set to: org.springframework.scheduling.quartz.SpringBeanJobFactory@67254c61


Using generated security password: bb85fafa-dc05-4723-9944-871b34daa316

Creating filter chain: Ant [pattern='/resources/**'], []
Creating filter chain: any request, [org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter@7c5aabdb, org.springframework.security.web.context.SecurityContextPersistenceFilter@4d3b2a89, org.springframework.security.web.header.HeaderWriterFilter@3e629a84, org.springframework.security.web.authentication.logout.LogoutFilter@22a6e53, org.springframework.security.web.savedrequest.RequestCacheAwareFilter@7b531742, org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter@7deadf43, org.springframework.security.web.authentication.AnonymousAuthenticationFilter@7671ce7a, org.springframework.security.web.session.SessionManagementFilter@3c26d940, org.springframework.security.web.access.ExceptionTranslationFilter@4edad3ea, org.springframework.security.web.access.intercept.FilterSecurityInterceptor@582333e7]
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 51
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
[Consumer clientId=consumer-4, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-4, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-4, groupId=community-consumer-group] (Re-)joining group
partitions assigned: [publish-0]
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-6, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-6, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-6, groupId=community-consumer-group] (Re-)joining group
Cluster ID: QO7D4advRRq2h8kiil_eJw
ConsumerConfig values: 
	auto.commit.interval.ms = 3000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = community-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version : 2.0.1
Kafka commitId : fa14705e51bd2ce5
Initializing ExecutorService
Starting Quartz Scheduler now
Cluster ID: QO7D4advRRq2h8kiil_eJw
[Consumer clientId=consumer-8, groupId=community-consumer-group] Discovered group coordinator DESKTOP-QPE2857:9092 (id: 2147483647 rack: null)
[Consumer clientId=consumer-8, groupId=community-consumer-group] Revoking previously assigned partitions []
partitions revoked: []
[Consumer clientId=consumer-8, groupId=community-consumer-group] (Re-)joining group
ClusterManager: detected 1 failed or restarted instances.
ClusterManager: Scanning for instance "DESKTOP-QPE28571605696205459"'s failed in-progress jobs.
Scheduler communityScheduler_$_DESKTOP-QPE28571605696304044 started.
Starting ProtocolHandler ["http-nio-8989"]
Tomcat started on port(s): 8989 (http) with context path '/community'
Started CommunityApplication in 7.205 seconds (JVM running for 8.389)
[Consumer clientId=consumer-2, groupId=community-consumer-group] Attempt to heartbeat failed since group is rebalancing
[Consumer clientId=consumer-2, groupId=community-consumer-group] Revoking previously assigned partitions [publish-0]
partitions revoked: [publish-0]
[Consumer clientId=consumer-2, groupId=community-consumer-group] (Re-)joining group
[Consumer clientId=consumer-2, groupId=community-consumer-group] Successfully joined group with generation 52
[Consumer clientId=consumer-8, groupId=community-consumer-group] Successfully joined group with generation 52
[Consumer clientId=consumer-4, groupId=community-consumer-group] Successfully joined group with generation 52
[Consumer clientId=consumer-6, groupId=community-consumer-group] Successfully joined group with generation 52
[Consumer clientId=consumer-2, groupId=community-consumer-group] Setting newly assigned partitions [publish-0]
[Consumer clientId=consumer-6, groupId=community-consumer-group] Setting newly assigned partitions [delete-0]
[Consumer clientId=consumer-8, groupId=community-consumer-group] Setting newly assigned partitions [share-0]
[Consumer clientId=consumer-4, groupId=community-consumer-group] Setting newly assigned partitions [comment-0, like-0, follow-0]
partitions assigned: [publish-0]
partitions assigned: [share-0]
partitions assigned: [comment-0, like-0, follow-0]
partitions assigned: [delete-0]
Initializing Spring DispatcherServlet 'dispatcherServlet'
Initializing Servlet 'dispatcherServlet'
Completed initialization in 6 ms
Starting without optional epoll library
Starting without optional kqueue library
load post list from DB.
在缓存中读取热度排行
load post list from DB.
load post list from DB.
